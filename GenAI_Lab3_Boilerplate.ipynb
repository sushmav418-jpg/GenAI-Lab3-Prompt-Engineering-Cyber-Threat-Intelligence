{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a0d862",
   "metadata": {},
   "source": [
    "# CASE STUDY: PROMPT ENGINEERING FOR CYBER THREAT INTELLIGENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb99fc4",
   "metadata": {},
   "source": [
    "Modern Security Operations Centers (SOCs) rely on automated systems to ingest and interpret real-time vulnerability intelligence feeds.\n",
    "\n",
    "However, Large Language Models (LLMs) frequently fall in:\n",
    "\n",
    "- Semantic version comparison\n",
    "- Interval boundary reasoning\n",
    "- Corporate mitigation nuance interpretation\n",
    "- Multi-document synthesis\n",
    "\n",
    "This case study investigates whether Prompt Engineering alone can transform a naïve LLM into a\n",
    "structured cyber threat reasoning assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c037b",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Contains 2018 CVE (Common Vulnerabilities and Exposures) data taken from National Vulnerability Database.\n",
    "2018 is chosen due to the presence of vendor statements for the CVEs which may help in giving a better response.\n",
    "\n",
    "The first task is of extraction:\n",
    "\n",
    "- Load the entire JSON data. (It is huge with millions of lines of code!)\n",
    "- You must implement an extraction script (extract_and_mask_target_cves) to isolate only the three target CVEs required for this lab.\n",
    "- Your script must actively extract crucial IDs such as descriptions, configurations and vendorComments and remove fields which may not contribute to our analysis.\n",
    "- Save this filtered, processed output as cve_dataset.json.\n",
    "- Once your dataset is generated, you need to write a utility to search cve_dataset.json for the matching 3 CVE IDs  and load only that specific JSON object into a masked_cve_json variable. By doing this, you will be able to rapidly test your prompt architectures against all three vulnerabilities without rewriting your code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_and_mask_cves(input_filename, output_filename):\n",
    "    pass\n",
    "INPUT_FILE = \"nvdcve-2.0-2018.json\" \n",
    "OUTPUT_FILE = \"cve_dataset.json\"\n",
    "\n",
    "extract_and_mask_cves(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b3cb9",
   "metadata": {},
   "source": [
    "# Models used in this lab\n",
    "\n",
    "We will be using Groq API in this lab. Choose any model of your choice from - https://console.groq.com/docs/rate-limits\n",
    "\n",
    "It gives multiple options with cloud-based models that are pretty good for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "from groq import Groq\n",
    "\n",
    "# Students will put in .env or secrets \n",
    "client = Groq(\n",
    "    api_key=\"\"\n",
    ")\n",
    "# The extracted JSON dataset\n",
    "with open(\"cve_dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# The specific CVE entries we are taking for evaluation\n",
    "target_cve_ids = [\n",
    "    \"CVE-2018-3810\", \n",
    "    \"CVE-2018-3814\", \n",
    "    \"CVE-2018-6523\"\n",
    "]\n",
    "\n",
    "# Create a new dictionary to hold the extracted CVEs\n",
    "masked_cve_json = {}\n",
    "\n",
    "# Iterate through the list of vulnerabilities\n",
    "for vuln in data.get(\"vulnerabilities\", []):\n",
    "    current_cve_id = vuln.get(\"cve\", {}).get(\"id\")\n",
    "    \n",
    "    # Check if this ID is in our target list\n",
    "    if current_cve_id in target_cve_ids:\n",
    "        masked_cve_json[current_cve_id] = vuln[\"cve\"]\n",
    "\n",
    "print(\"CVE JSON for Evaluation:\")\n",
    "print(json.dumps(masked_cve_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d91c15",
   "metadata": {},
   "source": [
    "## Zero shot \n",
    "\n",
    "Zero-shot prompting is an AI technique where models perform tasks without examples.\n",
    "\n",
    "You have to design the system prompt in such a way that it evaluates the json data. \n",
    "\n",
    "The SYSTEM prompt must include:\n",
    "- Extraction of internal asset profile.\n",
    "- Base your decision based on required fields (you need to identify which is relevant and specify them)\n",
    "- Output exactly three things: CLASSIFICATION: (SAFE, LOW, MEDIUM, or HIGHLY VULNERABLE), SCORE: (A risk score from 0 to 10)\n",
    "\n",
    "and REASONING: (Explaining your verdict based on the JSON fields)\n",
    "\n",
    "USER prompt must include the JSON data.\n",
    "\n",
    "Tune the parameters according to the task at end.\n",
    "\n",
    "For example: How many tokens should be in your response? (max_tokens)\n",
    "\n",
    "How much randomness you want in your response? (temperature)\n",
    "\n",
    "AND explore more of the params and give your reasoning behind those choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_zero_shot(cve_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d5db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cve_id, cve_data in masked_cve_json.items():\n",
    "    print(f\"Evaluating: {cve_id}\")\n",
    "    zero_shot_result = evaluate_zero_shot(cve_data)\n",
    "    \n",
    "    print(\"Zero-Shot Evaluation Result:\")\n",
    "    print(zero_shot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9479d",
   "metadata": {},
   "source": [
    "## One-shot\n",
    "\n",
    "One-shot prompting is a technique where a model is provided with a single example of a task before being asked to perform similar tasks. This approach is especially relevant for large language models (LLMs) and sits between zero-shot prompting (no examples) and few-shot prompting (multiple examples).\n",
    "\n",
    "\n",
    "\n",
    "The only difference here would be giving an example in the SYSTEM prompt like:\n",
    "\n",
    "-- EXAMPLE START --\n",
    "\n",
    "                        JSON DATA: {\"descriptions\": [{\"value\": \"SQL Injection in Oturia Plugin before 3.5 allows unauthenticated queries.\"}], \"configurations\": [{\"cpeMatch\": [{\"criteria\": \"cpe:2.3:a:oturia:smart_google_code_inserter:*:*:*:*:*:wordpress:*:*\", \"versionEndExcluding\": \"3.5\"}]}], \"vendorComments\": []}\n",
    "                        OUTPUT:\n",
    "                        CLASSIFICATION: HIGHLY VULNERABLE\n",
    "                        SCORE: 9.1/10\n",
    "                        REASONING: The 'configurations' field shows versions before 3.5 are vulnerable. The asset is v3.4.2, meaning it is mathematically within the vulnerable range. The 'descriptions' field indicates unauthenticated SQL queries (high impact). There is no 'vendorComments' field, meaning no mitigation exists.\n",
    "\n",
    "-- EXAMPLE END --\n",
    "\n",
    "We recommend to make your own example for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b940b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_shot(cve_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a396cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cve_id, cve_data in masked_cve_json.items():\n",
    "    print(f\"Evaluating: {cve_id}\")\n",
    "    one_shot_result = evaluate_one_shot(cve_data)\n",
    "    print(\"\\nOne-Shot Evaluation Result:\")\n",
    "    print(one_shot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcea1ee",
   "metadata": {},
   "source": [
    "## Few-shot\n",
    "\n",
    "Few-shot prompting is a technique where a model is provided with multiple examples of a task before being asked to perform similar tasks. \n",
    "\n",
    "The only difference here would be giving multiple such examples in the SYSTEM prompt with  variations like:\n",
    "\n",
    "-- EXAMPLE START --\n",
    "\n",
    "                        JSON DATA: {\"descriptions\": [{\"value\": \"SQL Injection in Oturia Plugin before 3.5 allows unauthenticated queries.\"}], \"configurations\": [{\"cpeMatch\": [{\"criteria\": \"cpe:2.3:a:oturia:smart_google_code_inserter:*:*:*:*:*:wordpress:*:*\", \"versionEndExcluding\": \"3.5\"}]}], \"vendorComments\": []}\n",
    "                        OUTPUT:\n",
    "                        CLASSIFICATION: HIGHLY VULNERABLE\n",
    "                        SCORE: 9.1/10\n",
    "                        REASONING: The 'configurations' field shows versions before 3.5 are vulnerable. The asset is v3.4.2, meaning it is mathematically within the vulnerable range. The 'descriptions' field indicates unauthenticated SQL queries (high impact). There is no 'vendorComments' field, meaning no mitigation exists.\n",
    "\n",
    "-- EXAMPLE END --\n",
    "\n",
    "-- EXAMPLE START --\n",
    "\n",
    "                    JSON DATA: {\"descriptions\": [{\"value\": \"In nProtect AVS V4.0 before 4.0.0.39, the driver file allows local users to cause a denial of service (BSOD).\"}], \"configurations\": [{\"cpeMatch\": [{\"criteria\": \"cpe:2.3:a:inca:nprotect_avs:*:*:*:*:*:*:*:*\", \"versionStartIncluding\": \"4.0\", \"versionEndExcluding\": \"4.0.0.39\"}]}], \"vendorComments\": [{\"comment\": \"The fixed version(V4.0.0.39) can be downloaded through the link below... [http://avsd.nprotect.net](http://avsd.nprotect.net)\"}]}\n",
    "                    OUTPUT:\n",
    "                    CLASSIFICATION: HIGHLY VULNERABLE\n",
    "                    SCORE: 8.2/10\n",
    "                    REASONING: The 'configurations' field establishes a vulnerable boundary of >= 4.0 and < 4.0.0.39. The asset is running version 4.0.0.35, which falls mathematically inside the vulnerable range. The 'vendorComments' provide a direct internet download link for a patch. However, the Internal Asset Profile explicitly states this workstation is air-gapped with NO internet access. Therefore, the vendor mitigation is void, and the asset remains fully exposed to a Denial of Service attack.\n",
    "-- EXAMPLE END --\n",
    "\n",
    "-- EXAMPLE START --\n",
    "\n",
    "                    JSON DATA: {\"descriptions\": [{\"value\": \"In Octopus Deploy versions 3.2.11 - 4.1.5 (fixed in 4.1.6), an authenticated user with ProcessEdit permission could reference an Azure account to bypass scoping restrictions, resulting in a potential escalation of privileges.\"}], \"configurations\": [{\"cpeMatch\": [{\"criteria\": \"cpe:2.3:a:octopus:octopus_deploy:*:*:*:*:*:*:*:*\", \"versionStartIncluding\": \"3.2.11\", \"versionEndIncluding\": \"4.1.5\"}]}], \"vendorComments\": []}\n",
    "                    OUTPUT:\n",
    "                    CLASSIFICATION: SAFE\n",
    "                    SCORE: 0.0/10\n",
    "                    REASONING: The 'configurations' field strictly bounds the vulnerability between versions 3.2.11 and 4.1.5 (inclusive). The internal asset is currently running version 4.1.8. Mathematically, 4.1.8 > 4.1.5, placing the asset completely outside the affected interval boundaries. Regardless of the 'descriptions' detailing privilege escalation, the software version is not susceptible to this specific CVE. \n",
    "-- EXAMPLE END --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ac226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_few_shot(cve_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cve_id, cve_data in masked_cve_json.items():\n",
    "    print(f\"Evaluating: {cve_id}\")\n",
    "    few_shot_result = evaluate_few_shot(cve_data)\n",
    "    print(\"\\nFew-Shot Evaluation Result:\")\n",
    "    print(few_shot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91055b48",
   "metadata": {},
   "source": [
    "## Chain of Thought (CoT) Prompting\n",
    "\n",
    "Chain of thought (CoT) is a prompt engineering technique that enhances the output of large language models (LLMs), particularly for complex tasks involving multistep reasoning. It facilitates problem-solving by guiding the model through a step-by-step reasoning process by using a coherent series of logical steps. \n",
    "\n",
    "\n",
    "You must include the required steps involved in generating the response. (IN THE SYSTEM PROMPT)\n",
    "\n",
    "And then mention it as \n",
    "\n",
    "Step 1: ...\n",
    "\n",
    "Step 2: .. \n",
    " \n",
    "and so on. (FINAL STEP BEING WHAT DO YOU WANT AS OUTPUT - CLASSIFICATION, SCORE, REASONING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "def evaluate_cot(cve_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198eb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cve_id, cve_data in masked_cve_json.items():\n",
    "    print(f\"Evaluating: {cve_id}\")\n",
    "    cot_result = evaluate_cot(cve_data)\n",
    "    print(\"\\nChain-of-Thought Evaluation Result:\")\n",
    "    print(cot_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7672fb",
   "metadata": {},
   "source": [
    "## Tree of Thought (ToT)\n",
    "\n",
    "Tree of Thought (ToT) prompting is a sophisticated prompt engineering technique for large language models (LLMs) that structures the reasoning process as a branching tree, enabling the model to explore, evaluate and refine multiple reasoning paths in parallel. This approach is designed to mimic human problem solving, where several options are considered, intermediate steps are evaluated and less promising directions are abandoned in favor of more fruitful ones.\n",
    "\n",
    "You must try this only for CVEs with vendorComments.\n",
    "\n",
    "One of the steps must include looking at different branches:\n",
    "\n",
    "- Branch A (Optimistic): Assume the 'vendorComments' (if present) provide a valid workaround that makes our specific asset safe. Calculate a hypothetical Score (1-4).\n",
    "- Branch B (Cynical): Assume the 'vendorComments' (if present) are just PR deflection or the workaround fails because of our specific Internal Asset Network Status. Calculate a hypothetical Score (8-10).\n",
    "- Branch C (Neutral): Assume the vulnerability requires complex local access, making the 'descriptions' impact moderate. Calculate a hypothetical Score (5-7).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d348f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "def evaluate_tot(cve_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_result = evaluate_tot(masked_cve_json)\n",
    "print(\"\\nTree-of-Thought Evaluation Result:\")\n",
    "print(tot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52edea3d",
   "metadata": {},
   "source": [
    "## Graph of Thought (GoT)\n",
    "\n",
    "Graph of Thought (GoT) Prompting is a framework that enhances large language models' (LLMs) reasoning capabilities by modeling thoughts as a dynamic, interconnected graph rather than a linear or tree-like structure.  Developed by researchers at ETH Zürich and published in August 2023, GoT represents LLM-generated ideas as vertices (nodes) and their relationships as edges.\n",
    "\n",
    "You will need to mention the nodes and edges to look at different relationships within the input data, in the steps of the system prompt.\n",
    "\n",
    "\n",
    "- Node 1 (CVE Threat): Extract the core threat from 'descriptions' and boundaries from 'configurations'. Estimate a Base Impact Score (1-10) based on the severity of the description.\n",
    "- Node 2 (Vendor Status): Extract mitigation status from 'vendorComments' (if present). Assign a Mitigation Modifier (e.g., -2 or -3 for a patch/workaround, 0 if no comment).\n",
    "- Node 3 (Internal Asset): State the extracted asset software/version and its Environmental Constraints (e.g., No Internet).\n",
    "\n",
    "- Edge A (Version Match): Does the Internal Asset (Node 3) fall inside the vulnerable bounds of the Threat (Node 1)? [True/False]. If False, Final Score automatically becomes 0.\n",
    "- Edge B (Mitigation Applicability): Can the mitigation in Node 2 be successfully applied given the environment constraints in Node 3? [True/False]. If False (e.g., requires a download but asset has no internet), the Mitigation Modifier from Node 2 is nullified (becomes 0).\n",
    "\n",
    "Create an output table of the format:\n",
    "| Asset | CVE | Vulnerable | Vendor Status | Action |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f64ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PHASE 4: GRAPH-OF-THOUGHT (GoT)\n",
    "# ==========================================\n",
    "import json\n",
    "\n",
    "def evaluate_got(cve_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad1b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cve_id, cve_data in masked_cve_json.items():\n",
    "    print(f\"Evaluating: {cve_id}\")\n",
    "    got_result = evaluate_got(cve_data)\n",
    "    print(\"\\n=== Phase 4: Graph-of-Thought Evaluation Result ===\")\n",
    "    print(got_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55cbea",
   "metadata": {},
   "source": [
    "## ReAct (Reason + Act) LOOP\n",
    "\n",
    "ReAct (Reasoning + Acting) Prompting is a technique used to improve AI models in solving problems. It combines two important processes i.e reasoning (thinking through the problem) and acting (taking actions based on that thinking). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b68dd7",
   "metadata": {},
   "source": [
    "Thought -> Action -> Observation Loop. You can keep max_iterations = 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PHASE 5: ReAct (Reason + Act) LOOP\n",
    "# ==========================================\n",
    "import json\n",
    "\n",
    "def evaluate_react(cve_data, max_steps=6):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56538fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_react(masked_cve_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304098b",
   "metadata": {},
   "source": [
    "## EXPLORATORY PROMPT ENGINEERING\n",
    "\n",
    "#### Technique 1: Self-Consistency Prompting (Majority Voting)\n",
    "\n",
    "This aims \"to replace the naive greedy decoding used in chain-of-thought prompting\". \n",
    "\n",
    "The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer. \n",
    "\n",
    "This helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoning.\n",
    "\n",
    "#### Technique 2: Prompt Decomposition (Micro-Prompts)\n",
    "\n",
    "Divide and conquer approach in prompting.\n",
    "\n",
    "#### Technique 3: Role-Based Prompting\n",
    "\n",
    "To better understand the context behind the domain of the questions.\n",
    "\n",
    "#### Technique 4: Reflection Prompting\n",
    "\n",
    "Reflection Prompting involves prompting the AI to analyze its own response before finalizing it. Instead of just generating an answer, the AI is guided to review its initial output, identify potential weaknesses, and suggest improvements. It’s similar to how a writer edits their own draft, playing both the role of the creator and the reviewer, resulting in higher-quality and more reliable outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PHASE 6: EXPLORATORY PROMPT ENGINEERING\n",
    "# ==========================================\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Standardized environmental constraint for all Phase 6 tests\n",
    "environmental_constraint = \"INTERNAL ASSET PROFILE: Highly sensitive local workstation (NO direct internet access).\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Technique 1: Self-Consistency Prompting (Majority Voting)\n",
    "# ---------------------------------------------------------\n",
    "def evaluate_self_consistency(cve_data, num_chains=5):\n",
    "    pass\n",
    "# ---------------------------------------------------------\n",
    "# Technique 2: Prompt Decomposition (Micro-Prompts)\n",
    "# ---------------------------------------------------------\n",
    "def evaluate_prompt_decomposition(cve_data):\n",
    "    pass\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Technique 3: Role-Based Prompting\n",
    "# ---------------------------------------------------------\n",
    "def evaluate_role_based(cve_data):\n",
    "    pass\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Technique 4: Reflection Prompting\n",
    "# ---------------------------------------------------------\n",
    "def evaluate_reflection(cve_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Execute Phase 6 (choose any 3)\n",
    "# ==========================================\n",
    "evaluate_self_consistency(masked_cve_json)\n",
    "evaluate_prompt_decomposition(masked_cve_json)\n",
    "evaluate_role_based(masked_cve_json)\n",
    "evaluate_reflection(masked_cve_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4801baa0",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "\n",
    "1. https://nvd.nist.gov\n",
    "2. https://www.redhat.com/en/topics/security/what-is-cve\n",
    "3. https://www.ibm.com/think/topics/cve\n",
    "4. https://console.groq.com/docs/models\n",
    "5. https://console.groq.com/docs/rate-limits\n",
    "6. https://console.groq.com/docs/quickstart\n",
    "7. Chain-of-Thought (CoT): Wei, J., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv. https://arxiv.org/abs/2201.11903\n",
    "\n",
    "8. Tree-of-Thought (ToT): Yao, S., et al. (2023). Tree of Thoughts: Deliberate Problem Solving with Large Language Models. arXiv. https://arxiv.org/abs/2305.10601\n",
    "\n",
    "9. Graph-of-Thought (GoT): Besta, M., et al. (2023). Graph of Thoughts: Solving Elaborate Problems with Large Language Models. arXiv. https://arxiv.org/abs/2308.09687\n",
    "\n",
    "10. ReAct (Reason + Act): Yao, S., et al. (2022). ReAct: Synergizing Reasoning and Acting in Language Models. arXiv. https://arxiv.org/abs/2210.03629\n",
    "\n",
    "11. Self-Consistency: Wang, X., et al. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv. https://arxiv.org/abs/2203.11171\n",
    "\n",
    "12. Reflection / Reflexion: Shinn, N., et al. (2023). Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv. https://arxiv.org/abs/2303.11366\n",
    "\n",
    "13. Common Vulnerability Scoring System (CVSS): Official specification by FIRST (Forum of Incident Response and Security Teams) detailing how base and environmental severity scores are calculated. https://www.first.org/cvss/\n",
    "\n",
    "14. Semantic Versioning (SemVer): The official specification for Major.Minor.Patch software versioning. https://semver.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
